import scipy.sparse as sps
import numpy as np
import gzip
import utils
import codecs
import pickle
import scipy
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.preprocessing import normalize
import scipy.sparse as sparse
import sys
import os

class Data:
	'''Class wrapper for functions needed to produce the data used in training, as well as evaluation. 
	Contains other important data related routines as static methods of the class.'''

	@staticmethod
	def read_sparse_input(lang_code, concepts_to_include, validation_set):
		'''Reads data for langauge lang_code from a file in the sparse matrix folder. 
		All of the concepts not found in the set concepts_to_include are excluded. 
		Aditionaly the concepts found in the validation_set are also excluded from the read data (even if they are found in concepts_to_include).'''

		rows = []
		columns = []
		data = []
		concept_id_2_index = {}

		intersection_with_concept_validation_set = set() # Sanity check

		with gzip.open(utils.get_sparse_matrix_file_path(lang_code), 'r') as f:
			# Used to check for repeated assignment of same cell (which should not happen)
			non_zero = set()
			# Used to check for word ID conformation
			vocabulary_size = utils.get_vocabulary_size(lang_code)

			documents_counter = 0
			for line in codecs.getreader("utf-8")(f):
				parts = line.split()

				if len(parts) != 3:
					raise Exception("Line formatting not as expected! Line: %s" % line)

				concept = parts[0]
				word_id = int(parts[1])

				if concept not in concepts_to_include:
					continue

				if concept in validation_set:
					intersection_with_concept_validation_set.add(concept)
					continue

				# Keeps track of which concept is described in the given document
				if concept not in concept_id_2_index:
					if documents_counter % 100000 == 0:
						print("%d documents processed" % documents_counter)
						sys.stdout.flush()

					concept_id_2_index[concept] = documents_counter
					documents_counter = documents_counter + 1

				r = concept_id_2_index[concept]

				if word_id > vocabulary_size:
					raise Exception('Word_ID higher then vocabulary size!')

				c = word_id
				d = int(parts[2])

				field = (r,c)
				if field in non_zero:
					# raise Exception('Repeated cell assignment!')
					print('Repeated cell assignment!')
					print(field)
				else:
					non_zero.add(field)

					rows.append(r)
					columns.append(c)
					data.append(d)

		index_2_concept_id = {index:concept_id for concept_id, index in concept_id_2_index.items()}

		# Generating the word index
		word_id_2_index = {}

		# Removes zero columns, by first generating a new index for the nonzero columns, and then updating the document representation to comply 
		# with the new index which contain only nonzero_columns 

		c = 0
		non_zero_word_ids = sorted(set(columns))
		for word_id in non_zero_word_ids:
			word_id_2_index[word_id] = c
			c += 1

		for i in range(len(columns)):
			word_id = columns[i]
			columns[i] = word_id_2_index[word_id]

		index_2_word_id = {index:word_id for word_id, index in word_id_2_index.items()}

		print("Intersection with validation set in %s counts %d elements" % (lang_code, len(intersection_with_concept_validation_set))) # Sanity check
		sys.stdout.flush()

		temp_processed_data = {'rows' : np.array(rows), 'columns' : np.array(columns), 'data' : np.array(data), 'concept_id_2_index' : concept_id_2_index, \
			'index_2_concept_id' : index_2_concept_id, 'word_id_2_index' : word_id_2_index, 'index_2_word_id' : index_2_word_id }

		return temp_processed_data

	# @staticmethod
	# def get_values_set_from_index_dict_per_lang(index_2_value_id_per_lang):
	# 	'''Returns the set of values per language, from an index_2_word_per_lang mapping or index_2_concept_per_lang mapping'''
	# 	lang_codes = index_2_value_id_per_lang.keys()
	# 	values_per_lang = {}
	# 	for lang_code in lang_codes:
	# 		values_per_lang[lang_code] = set(index_2_value_id_per_lang[lang_code].values())
	# 	return values_per_lang

	@staticmethod
	def get_values_set_from_index(index_2_value_id):
		'''Returns the set of values, from an index_2_word mapping or an index_2_concept mapping'''
		return set(index_2_value_id.values())

	@staticmethod
	def get_ordered_values_for_lang_from_index_dict(index_2_value_id):
		'''Returns the list of values, from an index_2_word mapping or an index_2_concept mapping order by their index'''
		return [index_2_value_id[ind] for ind in range(len(index_2_value_id))]

	@staticmethod
	def generate_intermediate_data_file(training_concepts_file_name, validation_set_file_name):
		'''Returns an intermediate data object (contains full vocabulary) generated by reading the articles about concepts that need to be included 
		(according in the training concepts file), and the ones that need to be excluded (according to the validation concepts file).'''

		training_concepts = pickle.load(open(utils.get_training_concepts_file_path(training_concepts_file_name), 'rb'))
		lang_codes = training_concepts['lang_codes']

		print('Number of documents to read and their overlap across languages:')
		print(training_concepts['overlap'])

		validation_concepts_per_lang = utils.get_validation_concepts_per_lang(validation_set_file_name)

		processed_data_per_lang = {}
		for lang_code in lang_codes:
			print("Processing language %s" % lang_code)
			sys.stdout.flush()

			concepts_to_include = training_concepts['to_keep_per_lang'][lang_code]
			val_concepts = validation_concepts_per_lang[lang_code]

			temp_processed_data = Data.read_sparse_input(lang_code, concepts_to_include, val_concepts)

			processed_data_per_lang[lang_code] = temp_processed_data

		dataset_dump_obj = {'training_concepts_file_name' : training_concepts_file_name, 'validation_set_file_name': validation_set_file_name, \
					'processed_data_per_lang': processed_data_per_lang, 'lang_codes' : lang_codes}
		pickle.dump(dataset_dump_obj, open(utils.get_training_dataset_file_path('{}_{}'.format(training_concepts_file_name, validation_set_file_name)), 'wb'), protocol = 4)
	
	@staticmethod
	def process_language_raw(lang_code, temp_processed_data, case_folding_flag, vocabulary_size, min_occ = 3):
		'''Returns a processed sparsed matrix generated using the data in the intermediate data object. The processing fixes the vocabulary 
		(based on the passed arguments) and removes the documents (if any) with no words inside the vocabulary of choice. 
		It additionaly provides a final index_2_word_id and index_2_concept_id. In the casefolded vocabulary setting, the word_id corresponds
		to the lowercase_id for the language provided by the get_vocabulary_mapping function defined in utils.py .'''

		print("Processing ", lang_code)
		
		rows = temp_processed_data['rows']
		columns = temp_processed_data['columns']
		data = temp_processed_data['data']
		index_2_word_id = temp_processed_data['index_2_word_id']
		index_2_concept_id = temp_processed_data['index_2_concept_id']
		
		assert(len(index_2_concept_id) == (max(rows) + 1))
		assert(len(index_2_word_id) == (max(columns)+ 1))

		if case_folding_flag:
			# Assumes that word_ids are given as lowercase_ids

			# From column index 2 word id 
			for i in range(len(columns)):
				index = columns[i]
				columns[i] = index_2_word_id[index]
				
			word_id_2_lowercase_id, lowercase_word_2_lowercase_id, lowercase_id_2_lowercase_word = \
					utils.get_vocabulary_mapping(lang_code, True)
				
			# From word id 2 lowercase word id 
			for i in range(len(columns)):
				word_id = columns[i]
				columns[i] = word_id_2_lowercase_id[word_id]
				
			# Generating a word index on top of lowercase word ids 
			lowercase_word_id_2_index = {}

			c = 0
			non_zero_word_ids = sorted(set(columns))
			for lowercase_word_id in non_zero_word_ids:
				lowercase_word_id_2_index[lowercase_word_id] = c
				c += 1
				
			for i in range(len(columns)):
				lowercase_word_id = columns[i]
				columns[i] = lowercase_word_id_2_index[lowercase_word_id]

			index_2_word_id = {index:word_id for word_id, index in lowercase_word_id_2_index.items()}

		current_number_of_documents = len(index_2_concept_id)
		current_vocabulary_size = len(index_2_word_id)
		
		temp_Z = sparse.coo_matrix((data, (rows, columns)), dtype=np.float64, shape=(current_number_of_documents, current_vocabulary_size))
		temp_Z = temp_Z.tocoo()
		
		print("Filtering to only %d most frequent words" % vocabulary_size)
		sys.stdout.flush()
		##### Keep most frequent k words
		nnz_per_column = utils.get_nnz(temp_Z, 'col')
		sorted_by_frequency = np.argsort(nnz_per_column)

		most_frequent = np.sort(sorted_by_frequency[-vocabulary_size:])
		idx_to_drop = sorted_by_frequency[:-vocabulary_size]

		# Remove columns; Update index and current vocabulary size
		temp_Z = utils.remove_columns(temp_Z, idx_to_drop)
		index_2_word_id = utils.update_index_after_removal(index_2_word_id, idx_to_drop)
		current_vocabulary_size -= len(idx_to_drop)
		assert(temp_Z.shape[1] == current_vocabulary_size)
		print("Temp data rep matrix {} for language {}. Desired vocabulary {}".format(temp_Z.shape, lang_code, vocabulary_size))
		
		print("Filtering to only words occurring in at least %d documents" % min_occ)
		sys.stdout.flush()
		#### Keep only words occuring in at least d documents
		nnz_per_column = utils.get_nnz(temp_Z, 'col')
		idx_to_drop = np.where(nnz_per_column < min_occ)[0]

		if len(idx_to_drop) == 0:
			print("No words occuring in less then %d documents." % min_occ)
		else:
			# Remove columns; Update index and current vocabulary size
			temp_Z = utils.remove_columns(temp_Z, idx_to_drop)
			index_2_word_id = utils.update_index_after_removal(index_2_word_id, idx_to_drop)
			current_vocabulary_size -= len(idx_to_drop)
			assert(current_vocabulary_size == temp_Z.shape[1])
			print("Removed %d words" % len(idx_to_drop))
		
		print("Removing documents with no words in vocabulary")
		sys.stdout.flush()
		# Remove documents that have no words in vocabulary
		nnz_per_row = utils.get_nnz(temp_Z, 'row')
		idx_to_drop = np.where(nnz_per_row == 0)[0]

		if len(idx_to_drop) == 0:
			print("All documents contain at least one word from the vocabulary.")
			sys.stdout.flush()
		else:
			# Remove rows (remove_rows acts in-place); Update index and current number of documents
			utils.remove_rows(temp_Z, idx_to_drop)
			index_2_concept_id = utils.update_index_after_removal(index_2_concept_id, idx_to_drop)
			current_number_of_documents -= len(idx_to_drop)
			assert(current_number_of_documents == temp_Z.shape[0])
			print("Removed %d documents" % len(idx_to_drop))

		print("Final matrix for %s has shape: " % lang_code, temp_Z.shape)
		
		return index_2_concept_id, index_2_word_id, temp_Z

	@staticmethod
	def generate_intermediate_validation_data_file(validation_set_file_name, test_set_flag, target_concepts_suffix):
		'''Returns an intermediate data object (contains full vocabulary) generated by reading the articles about concepts that need to be included 
		in validation (according to the target concepts and validation_concepts files).'''

		validation_set_dump_obj = pickle.load(open(utils.get_validation_concepts_file_path(validation_set_file_name), 'rb'))
		validation_set_file_name_extended = utils.get_extended_validation_set_file_name(validation_set_file_name, test_set_flag, target_concepts_suffix)

		validation_concepts_per_lang = utils.get_filtered_validation_concepts_per_lang(validation_set_file_name, test_set_flag)

		if 'lang_codes_to_include' in validation_set_dump_obj['params']:
			lang_codes = validation_set_dump_obj['params']['lang_codes_to_include']
		else:
			lang_codes = validation_set_dump_obj['lang_codes']

		target_concepts_per_lang = utils.get_target_concepts_per_lang(lang_codes, target_concepts_suffix)

		processed_data_per_lang = {}
		for lang_code in lang_codes:
			print("Processing language %s" % lang_code)
			sys.stdout.flush()

			target_concepts = target_concepts_per_lang[lang_code]
			queries = validation_concepts_per_lang[lang_code]
			concepts_to_include = target_concepts.union(queries)

			temp_processed_data = Data.read_sparse_input(lang_code, concepts_to_include, set())
			processed_data_per_lang[lang_code] = temp_processed_data

		dataset_dump_obj = {'validation_set_file_name' : validation_set_file_name, \
					'processed_data_per_lang': processed_data_per_lang, 'lang_codes' : lang_codes}

		pickle.dump(dataset_dump_obj, open(utils.get_validation_datasets_file_path(validation_set_file_name_extended), 'wb'))
	
	@staticmethod
	def process_validation_language_raw(lang_code, temp_processed_data, emb_index_2_word_id, case_folding_flag, vocabulary_size):
		'''Returns a processed sparsed matrix generated using the data in the intermediate data object. The processing fixes the vocabulary 
		(based on the embeddings available from the training phase) and removes the documents (if any) with no words inside the defined vocabulary. 
		It additionaly provides a final index_2_word_id and index_2_concept_id. In the casefolded vocabulary setting, the word_id corresponds
		to the lowercase_id for the language provided by the get_vocabulary_mapping function defined in utils.py .'''
		print("Processing ", lang_code)
		
		vocabulary = Data.get_values_set_from_index(emb_index_2_word_id)

		rows = temp_processed_data['rows']
		columns = temp_processed_data['columns']
		data = temp_processed_data['data']
		val_index_2_word_id = temp_processed_data['index_2_word_id']
		val_index_2_concept_id = temp_processed_data['index_2_concept_id']

		assert(len(val_index_2_concept_id) == (max(rows) + 1))
		assert(len(val_index_2_word_id) == (max(columns)+ 1))

		if case_folding_flag:
			# Assumes that word_ids are given as lowercase_ids

			# From column index 2 word id 
			for i in range(len(columns)):
				index = columns[i]
				columns[i] = val_index_2_word_id[index]
				
			word_id_2_lowercase_id, lowercase_word_2_lowercase_id, lowercase_id_2_lowercase_word = \
					utils.get_vocabulary_mapping(lang_code, True)
				
			# From word id 2 lowercase word id 
			for i in range(len(columns)):
				word_id = columns[i]
				columns[i] = word_id_2_lowercase_id[word_id]
				
			# Generating a word index on top of lowercase word ids 
			lowercase_word_id_2_index = {}

			c = 0
			non_zero_word_ids = sorted(set(columns))
			for lowercase_word_id in non_zero_word_ids:
				lowercase_word_id_2_index[lowercase_word_id] = c
				c += 1
				
			for i in range(len(columns)):
				lowercase_word_id = columns[i]
				columns[i] = lowercase_word_id_2_index[lowercase_word_id]

			val_index_2_word_id = {index:word_id for word_id, index in lowercase_word_id_2_index.items()}

		current_number_of_documents = len(val_index_2_concept_id)
		current_vocabulary_size = len(val_index_2_word_id)

		temp_Z = sparse.coo_matrix((data, (rows, columns)), dtype=np.float64, shape=(current_number_of_documents, current_vocabulary_size))
		temp_Z = temp_Z.tocoo()
		
		print("Filtering out words for which embeddings are not available")
		sys.stdout.flush()
		# Remove words not having an embedding
		val_ordered_word_ids = Data.get_ordered_values_for_lang_from_index_dict(val_index_2_word_id)
		idx_to_drop = np.where([word_id not in vocabulary for word_id in val_ordered_word_ids])[0]

		if len(idx_to_drop) == 0:
			print("Full vocabulary was matched")
			sys.stdout.flush()
		else:
			temp_Z = utils.remove_columns(temp_Z, idx_to_drop)
			val_index_2_word_id = utils.update_index_after_removal(val_index_2_word_id, idx_to_drop)
			current_vocabulary_size -= len(idx_to_drop)
			assert(temp_Z.shape[1] == current_vocabulary_size)
			print("Removed %d unmatched words. Kept %d words" % (len(idx_to_drop), current_vocabulary_size))

		if current_vocabulary_size > vocabulary_size:
			print("Filtering to only %d most frequent words" % vocabulary_size)
			sys.stdout.flush()
			##### Keep most frequent k words
			nnz_per_column = utils.get_nnz(temp_Z, 'col')
			sorted_by_frequency = np.argsort(nnz_per_column)

			most_frequent = np.sort(sorted_by_frequency[-vocabulary_size:])
			idx_to_drop = sorted_by_frequency[:-vocabulary_size]

			# Remove columns; Update index and current vocabulary size
			temp_Z = utils.remove_columns(temp_Z, idx_to_drop)
			val_index_2_word_id = utils.update_index_after_removal(val_index_2_word_id, idx_to_drop)
			current_vocabulary_size -= len(idx_to_drop)
			assert(temp_Z.shape[1] == vocabulary_size)
			assert(current_vocabulary_size == vocabulary_size)

		print("Removing documents with no words in vocabulary")
		sys.stdout.flush()
		# Remove documents that have no words in vocabulary
		nnz_per_row = utils.get_nnz(temp_Z, 'row')
		idx_to_drop = np.where(nnz_per_row == 0)[0]

		if len(idx_to_drop) == 0:
			print("All documents contain at least one word from the vocabulary.")
			removed_zero_word_concepts = set()
			sys.stdout.flush()
		else:
			# Remove rows (remove_rows acts in-place)
			# Note deleted concepts
			removed_zero_word_concepts = set([val_index_2_concept_id[idx] for idx in idx_to_drop])

			#Update index and current number of documents
			utils.remove_rows(temp_Z, idx_to_drop)
			val_index_2_concept_id = utils.update_index_after_removal(val_index_2_concept_id, idx_to_drop)
			current_number_of_documents -= len(idx_to_drop)
			assert(current_number_of_documents == temp_Z.shape[0])
			print("Removed %d documents" % len(idx_to_drop))

		# Match vocabulary and reorder indices
		# Every remaining word in validation has an embedding on some index
		emb_word_id_2_index = {word_id:emb_idx for emb_idx, word_id in emb_index_2_word_id.items()}
		val_ordered_word_ids = Data.get_ordered_values_for_lang_from_index_dict(val_index_2_word_id)

		# Map each word_id from validation_set to its embedding
		# val_index_2_emb_index = {idx:emb_word_id_2_index[val_ordered_word_ids[idx]] for idx in range(len(val_ordered_word_ids))}

		ordered_emb_idx = [emb_word_id_2_index[val_ordered_word_ids[idx]] for idx in range(len(val_ordered_word_ids))]

		return val_index_2_concept_id, val_index_2_word_id, ordered_emb_idx, temp_Z, removed_zero_word_concepts


	@staticmethod
	def construct_Z(Z_per_lang, lang_codes):
		'''Returns a concatenation of all the sparse matrices representing document per language, in order to get a block diagonal representation 
		that covers all of the documents in all languages with their respective vocabularies, which is needed in training.'''
		mats = [Z_per_lang[lang_code] for lang_code in lang_codes]
		Z = sparse.block_diag(mats, format='csr', dtype=np.float64)

		# Sanity checks
		# Every word occurs occurrs in at least three documents
		nnz_per_column = utils.get_nnz(Z, 'col')
		idx_to_drop = np.where(nnz_per_column < 3)[0]
		assert (len(idx_to_drop) == 0)

		# Every document describes exactly one concept
		nnz_per_row = utils.get_nnz(Z, 'row')
		idx_to_drop = np.where(nnz_per_row == 1)[0]
		assert (len(idx_to_drop) == 0)

		return Z

	@staticmethod
	def get_concept_id_per_row(index_2_concept_id_per_lang, lang_codes):
		'''Returns a dictionary that maps concept_ids with rows in the Z matrix.'''
		concept_id_per_row = []
		for lang_code in lang_codes:
			index_2_concept_id_temp = index_2_concept_id_per_lang[lang_code]
			concept_id_per_row = concept_id_per_row + \
				[index_2_concept_id_temp[i] for i in range(len(index_2_concept_id_temp))]
				
		return concept_id_per_row

	@staticmethod
	def total_number_of_documents(index_2_concept_id_per_lang, lang_codes):
		'''Returns the number of rows in the Z matrix.'''
		total = 0
		for lang_code in lang_codes:
			total += len(index_2_concept_id_per_lang[lang_code])
		
		return total

	@staticmethod
	def total_vocabulary_size(index_2_word_id_per_lang, lang_codes):
		'''Returns the number of columns in the Z matrix.'''
		total = 0
		for lang_code in lang_codes:
			total += len(index_2_word_id_per_lang[lang_code])
			
		return total

	@staticmethod
	def get_embeddings_per_lang(lang_codes, index_2_word_id_per_lang, embeddings):
		'''Returns the trained embeddings per lang, given the embeddings in all languages and the index_2_word_id mapping per language.'''
		embeddings = np.array(embeddings)

		current_pos = 0
		embs_per_lang = {}
		for ind in range(len(lang_codes)):
			lang_code = lang_codes[ind]
			voc_size = len(index_2_word_id_per_lang[lang_code])

			embs = embeddings[current_pos:(current_pos+voc_size)]
			embs_per_lang[lang_code] = embs

			current_pos += voc_size

		return embs_per_lang

	# Note: For concept (or word deletion), remove it from the per language matrix, update concept (or word) index in langauge, reconstruct Z (or/and Y)
	@staticmethod
	def construct_Y(index_2_concept_id_per_lang, lang_codes):
		'''Returns a one hot representation in each row for the concept that the article in the corresponding row in Z describes'''
		concept_id_per_row = Data.get_concept_id_per_row(index_2_concept_id_per_lang, lang_codes)

		c = 0
		concept_id_2_Y_col_index = {}
		all_concepts_set = sorted(set(concept_id_per_row))
		for concept_id in all_concepts_set:
			concept_id_2_Y_col_index[concept_id] = c
			c += 1
			
		Y_cols = [concept_id_2_Y_col_index[concept_id_per_row[i]] for i in range(len(concept_id_per_row))]
		Y_rows = range(len(Y_cols))
		Y_data = np.ones_like(Y_rows)
		Y_COO = sps.coo_matrix((Y_data, (Y_rows, Y_cols)), dtype=np.int32, \
				shape=(Data.total_number_of_documents(index_2_concept_id_per_lang, lang_codes), len(concept_id_2_Y_col_index)))
		Y = Y_COO.asformat('csr')
		
		# Sanity checks
		# Every concept is described in at least one document
		nnz_per_column = utils.get_nnz(Y, 'col')
		idx_to_drop = np.where(nnz_per_column == 0)[0]
		assert (len(idx_to_drop) == 0)
		
		# Every document describes exactly one concept
		nnz_per_row = utils.get_nnz(Y, 'row')
		idx_to_drop = np.where(nnz_per_row != 1)[0]
		assert (len(idx_to_drop) == 0)

		return Y

	def __init__(self):
		pass

	def load_validation(self, results_obj_dump_name, vocabulary_size, test_set_flag, target_concepts_suffix, validation_set_file_name = None):
		'''Loads the data needed to evaluate the results_obj corresponding to the argument results_obj_dump_name, 
		into the data object it is called upon.'''
		results_dump_obj = pickle.load(open(utils.get_results_dump_file_path(results_obj_dump_name), 'rb'))
		self.results_dump_obj = results_dump_obj

		training_dataset_name = results_dump_obj['data']
		with open(utils.get_training_dataset_file_path(training_dataset_name), 'rb') as f:
			training_dataset_dump_obj = pickle.load(f)

		if validation_set_file_name == None:
			validation_set_file_name = training_dataset_dump_obj['validation_set_file_name']

		self.validation_set_file_name = validation_set_file_name
		self.test_set_flag = test_set_flag

		assert(os.path.isfile(utils.get_validation_concepts_file_path(validation_set_file_name)))

		with open(utils.get_validation_concepts_file_path(validation_set_file_name), 'rb') as f:
			validation_set_dump_obj = pickle.load(f)
		

		if 'lang_codes_to_include' in validation_set_dump_obj['params']:
			validation_lang_codes = validation_set_dump_obj['params']['lang_codes_to_include']
		else:
			validation_lang_codes = validation_set_dump_obj['lang_codes']

		lang_codes = list(set(training_dataset_dump_obj['lang_codes']) & set(validation_lang_codes))

		assert(len(lang_codes) != 0)

		validation_set_file_name_extended = utils.get_extended_validation_set_file_name(validation_set_file_name, test_set_flag, target_concepts_suffix)
		validaiton_dataset_name = '{}_{}'.format(validation_set_file_name_extended, training_dataset_name)
		print("Validation dataset file name - extended", validation_set_file_name_extended)
		print("Validation dataset full name", validaiton_dataset_name)

		self.emb_index_2_word_id_per_lang = training_dataset_dump_obj['index_2_word_id_per_lang']
		self.emb_case_folding_flag = training_dataset_dump_obj['case_folding_flag']
		self.idf_per_lang = training_dataset_dump_obj['idf_per_lang']

		self.embeddings = results_dump_obj['training_outcome']['M2_e_vecs']
		self.e_vals = results_dump_obj['training_outcome']['M2_e_vals']

		if os.path.isfile(utils.get_validation_datasets_file_path(validaiton_dataset_name)):
			print("Final validation dataset file already generated. Exists as: ", validaiton_dataset_name)
			sys.stdout.flush()
		else:
			# if multiple validation sets are needed update the following if
			if not os.path.isfile(utils.get_validation_datasets_file_path(validation_set_file_name_extended)):
				print("Generating intermediate validation dataset.")
				sys.stdout.flush()
				self.generate_intermediate_validation_data_file(validation_set_file_name, test_set_flag, target_concepts_suffix)

			print('Generating validation documents\' representations')
			sys.stdout.flush()
			with open(utils.get_validation_datasets_file_path(validation_set_file_name_extended), 'rb') as f:
				intermediate_validation_dataset_dump_obj = pickle.load(f)
			processed_data_per_lang = intermediate_validation_dataset_dump_obj['processed_data_per_lang']

			val_index_2_word_id_per_lang = {}
			val_index_2_concept_id_per_lang = {} 
			removed_zero_word_concepts_per_lang = {}
			Z_per_lang = {}
			idfed_Z_per_lang = {}
			ordered_emb_idx_per_lang = {}

			for lang_code in lang_codes:
				val_index_2_concept_id, val_index_2_word_id, ordered_emb_idx, temp_Z, removed_zero_word_concepts = \
					self.process_validation_language_raw(lang_code, processed_data_per_lang[lang_code], self.emb_index_2_word_id_per_lang[lang_code], self.emb_case_folding_flag, vocabulary_size = vocabulary_size)

				val_index_2_concept_id_per_lang[lang_code] = val_index_2_concept_id
				val_index_2_word_id_per_lang[lang_code] = val_index_2_word_id
				ordered_emb_idx_per_lang[lang_code] = ordered_emb_idx
				removed_zero_word_concepts_per_lang[lang_code] = removed_zero_word_concepts
				Z_per_lang[lang_code] = temp_Z

				# These idf_ed Z are not actually used
				# The idf step is done during the document embedings generation step in the evaluator class
				idf = scipy.sparse.diags(self.idf_per_lang[lang_code][ordered_emb_idx], format='csc')
				idfed_Z = temp_Z.dot(idf)
				idfed_Z_per_lang[lang_code] = idfed_Z

			final_dataset_dump_obj = {'idfed_Z_per_lang' : idfed_Z_per_lang, 'ordered_emb_idx_per_lang' : ordered_emb_idx_per_lang, \
				'index_2_word_id_per_lang': val_index_2_word_id_per_lang, 'index_2_concept_id_per_lang' : val_index_2_concept_id_per_lang, \
				'Z_per_lang' : Z_per_lang, 'removed_zero_word_concepts_per_lang' : removed_zero_word_concepts_per_lang, 'lang_codes' : lang_codes, \
				'case_folding_flag' : self.emb_case_folding_flag, 'training_dataset_dump_name' : training_dataset_name}

			with open(utils.get_validation_datasets_file_path(validaiton_dataset_name), 'wb') as f:
				pickle.dump(final_dataset_dump_obj, f)


		with open(utils.get_validation_datasets_file_path(validaiton_dataset_name), 'rb') as f:
			final_dataset_dump_obj = pickle.load(f)

		self.val_case_folding_flag = final_dataset_dump_obj['case_folding_flag']
		self.val_index_2_concept_id_per_lang = final_dataset_dump_obj['index_2_concept_id_per_lang']
		self.val_index_2_word_id_per_lang = final_dataset_dump_obj['index_2_word_id_per_lang']
		self.Z_per_lang = final_dataset_dump_obj['Z_per_lang']

		# These document embeddings are not actually used
		# The used document embeddings are generated in the evaluator class
		assert(self.val_case_folding_flag == self.emb_case_folding_flag)
		# ordered_emb_idx_per_lang = final_dataset_dump_obj['ordered_emb_idx_per_lang']
		# idfed_Z_per_lang = final_dataset_dump_obj['idfed_Z_per_lang']
		# doc_embs_per_lang = {}

		# emb_index_2_word_id_per_lang = training_dataset_dump_obj['index_2_word_id_per_lang']
		# embs_per_lang = self.get_embeddings_per_lang(training_dataset_dump_obj['lang_codes'], emb_index_2_word_id_per_lang, results_dump_obj['training_outcome']['M2_e_vecs'])

		# for lang_code in lang_codes:
		# 	embs = embs_per_lang[lang_code][ordered_emb_idx_per_lang[lang_code]]
		# 	idfed_temp_Z = idfed_Z_per_lang[lang_code]

		# 	doc_embs = idfed_temp_Z.dot(embs)
		# 	doc_embs_per_lang[lang_code] = doc_embs

		# self.doc_embs_per_lang = doc_embs_per_lang
		###

		self.removed_zero_word_concepts_per_lang = final_dataset_dump_obj['removed_zero_word_concepts_per_lang']
		self.lang_codes = final_dataset_dump_obj['lang_codes']
		self.training_languages = training_dataset_dump_obj['lang_codes']
		if test_set_flag:
			self.validation_set = utils.get_test_concepts_from_validation_set(validation_set_file_name)
		else:
			self.validation_set = utils.get_val_concepts_from_validation_set(validation_set_file_name)

	def load_training(self, training_concepts_file_name, validation_set_file_name, case_folding_flag, vocabulary_size):
		'''Loads the data needed to train according to the training and validation concepts defined in the files 
		training_concepts_file_name and validation_set_file_name. If the case_folding_flag is set to True, the data is lowercased 
		(consequently the used vocabulary is lowercased), otherwise the data is considered in its original form.'''

		intermediate_dataset_dump_name = '{}_{}'.format(training_concepts_file_name, validation_set_file_name)
		final_dataset_dump_name = '{}_{}'.format(intermediate_dataset_dump_name, 'casefolded' if case_folding_flag else 'non_casefolded')

		assert(os.path.isfile(utils.get_training_concepts_file_path(training_concepts_file_name)))
		assert(os.path.isfile(utils.get_validation_concepts_file_path(validation_set_file_name)))

		
		if os.path.isfile(utils.get_training_dataset_file_path(final_dataset_dump_name)):
			print("Final dataset file already generated. Exists as: ", final_dataset_dump_name)
			sys.stdout.flush()
		else:
			if not os.path.isfile(utils.get_training_dataset_file_path(intermediate_dataset_dump_name)):
				print("Generating intermediate dataset.")
				sys.stdout.flush()
				self.generate_intermediate_data_file(training_concepts_file_name, validation_set_file_name)

			print("Generating final dataset.")
			sys.stdout.flush()
			intermediate_dataset_dump_obj = pickle.load(open(utils.get_training_dataset_file_path(intermediate_dataset_dump_name), 'rb'))
			processed_data_per_lang = intermediate_dataset_dump_obj['processed_data_per_lang']
			lang_codes = intermediate_dataset_dump_obj['lang_codes']


			index_2_concept_id_per_lang = {} 
			index_2_word_id_per_lang = {}
			Z_per_lang = {}
			idf_per_lang = {}
			pre_norm_Z = {}

			for lang_code in lang_codes:
				index_2_concept_id, index_2_word_id, temp_Z = self.process_language_raw(lang_code, processed_data_per_lang[lang_code], case_folding_flag, vocabulary_size = vocabulary_size)
				index_2_concept_id_per_lang[lang_code] = index_2_concept_id
				index_2_word_id_per_lang[lang_code] = index_2_word_id
				pre_norm_Z[lang_code] = temp_Z

				transformer = TfidfTransformer(norm = 'l2', smooth_idf=False)
				idfed_temp_Z = transformer.fit_transform(temp_Z)
				idf = transformer.idf_

				Z_per_lang[lang_code] = idfed_temp_Z
				idf_per_lang[lang_code] = idf


			final_dataset_dump_obj = {'pre_norm_Z' : pre_norm_Z, 'training_concepts_file_name' : training_concepts_file_name, 'intermediate_dataset_dump_name' : intermediate_dataset_dump_name, \
				'validation_set_file_name': validation_set_file_name, 'lang_codes' : lang_codes, 'index_2_concept_id_per_lang' : index_2_concept_id_per_lang, \
				'index_2_word_id_per_lang' : index_2_word_id_per_lang, 'Z_per_lang' : Z_per_lang, 'idf_per_lang' : idf_per_lang, 'case_folding_flag' : case_folding_flag}

			pickle.dump(final_dataset_dump_obj, open(utils.get_training_dataset_file_path(final_dataset_dump_name), 'wb'), protocol = 4)

		print("Constructing Z and Y matrices")
		sys.stdout.flush()
		final_dataset_dump_obj = pickle.load(open(utils.get_training_dataset_file_path(final_dataset_dump_name), 'rb'))
		Z_per_lang = final_dataset_dump_obj['Z_per_lang']
		lang_codes = final_dataset_dump_obj['lang_codes']
		index_2_concept_id_per_lang = final_dataset_dump_obj['index_2_concept_id_per_lang']

		self.lang_codes = final_dataset_dump_obj['lang_codes']
		self.final_dataset_dump_name = final_dataset_dump_name
		self.Z = self.construct_Z(Z_per_lang, lang_codes)
		self.Z_T = sps.csr_matrix.transpose(self.Z).asformat('csr')
		self.Y = self.construct_Y(index_2_concept_id_per_lang, lang_codes)
		print("Z shape:", self.Z.shape)
		print("Y shape", self.Y.shape)